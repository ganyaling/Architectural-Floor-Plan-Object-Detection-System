"""
PyTorch Faster R-CNN æ¨ç†è„šæœ¬
ç”¨äºå•å¼ æˆ–æ‰¹é‡ç›®æ ‡æ£€æµ‹æ¨ç†
"""

import argparse
import os
import sys
from pathlib import Path
from typing import List, Dict, Tuple
import json

import torch
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.transforms import transforms
from tqdm import tqdm
import pycocotools.coco as coco
from pycocotools.cocoeval import COCOeval


class DetectionInference:
    """ç›®æ ‡æ£€æµ‹æ¨ç†å·¥å…·"""
    
    # ç±»åˆ« ID åˆ°åç§°çš„æ˜ å°„
    CLASSES = {
        0: 'background',
        1: 'wall',
        2: 'room'
    }
    
    # é¢œè‰²æ˜ å°„
    COLORS = {
        'wall': 'red',
        'room': 'blue',
        'background': 'green'
    }
    
    def __init__(self, checkpoint_path: str, device: str = 'cuda:0', conf_threshold: float = 0.5):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            device: è®¾å¤‡ï¼ˆ'cuda:0' æˆ– 'cpu'ï¼‰
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        """
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.conf_threshold = conf_threshold
        
        print(f"âœ… ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"âœ… ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold}")
        
        # åŠ è½½æ¨¡å‹
        self.model = fasterrcnn_resnet50_fpn(pretrained=False, num_classes=3)
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        if 'model' in checkpoint:
            self.model.load_state_dict(checkpoint['model'])
        else:
            self.model.load_state_dict(checkpoint)
        
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {checkpoint_path}")
    
    def preprocess(self, image: Image.Image) -> torch.Tensor:
        """é¢„å¤„ç†å›¾åƒ"""
        transform = transforms.ToTensor()
        return transform(image).to(self.device)
    
    def infer_single(self, image_path: str, score_threshold: float = None) -> Dict:
        """
        å•å¼ å›¾åƒæ¨ç†
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            score_threshold: åˆ†æ•°é˜ˆå€¼ï¼ˆå¦‚æœä¸º None ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        
        Returns:
            æ¨ç†ç»“æœå­—å…¸
        """
        if score_threshold is None:
            score_threshold = self.conf_threshold
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        img_tensor = self.preprocess(image)
        
        # æ¨ç†
        with torch.no_grad():
            predictions = self.model([img_tensor])
        
        pred = predictions[0]
        
        # ç­›é€‰é«˜ç½®ä¿¡åº¦æ£€æµ‹
        mask = pred['scores'] >= score_threshold
        
        results = {
            'image_path': str(image_path),
            'image_size': image.size,  # (width, height)
            'detections': []
        }
        
        for box, label, score in zip(
            pred['boxes'][mask],
            pred['labels'][mask],
            pred['scores'][mask]
        ):
            x1, y1, x2, y2 = box.cpu().numpy().astype(float)
            label_id = label.item()
            confidence = score.item()
            
            results['detections'].append({
                'bbox': [x1, y1, x2, y2],
                'category': label_id,
                'category_name': self.CLASSES.get(label_id, 'unknown'),
                'confidence': confidence,
                'width': x2 - x1,
                'height': y2 - y1
            })
        
        return results
    
    def infer_batch(self, image_dir: str, output_json: str = None) -> List[Dict]:
        """
        æ‰¹é‡æ¨ç†
        
        Args:
            image_dir: å›¾åƒç›®å½•
            output_json: ç»“æœä¿å­˜ JSON æ–‡ä»¶è·¯å¾„
        
        Returns:
            æ¨ç†ç»“æœåˆ—è¡¨
        """
        image_dir = Path(image_dir)
        image_files = list(image_dir.glob('**/*.png')) + list(image_dir.glob('**/*.jpg'))
        
        print(f"\nğŸ” æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒ")
        
        all_results = []
        
        for img_path in tqdm(image_files, desc="æ¨ç†ä¸­"):
            try:
                results = self.infer_single(str(img_path))
                all_results.append(results)
            except Exception as e:
                print(f"âš ï¸  å¤„ç†å¤±è´¥: {img_path} - {e}")
        
        # ä¿å­˜ç»“æœ
        if output_json:
            with open(output_json, 'w') as f:
                json.dump(all_results, f, indent=2)
            print(f"âœ… ç»“æœå·²ä¿å­˜: {output_json}")
        
        return all_results
    
    def draw_predictions(self, image_path: str, output_path: str = None, 
                        score_threshold: float = None) -> Image.Image:
        """
        ç»˜åˆ¶æ£€æµ‹ç»“æœ
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            output_path: è¾“å‡ºå›¾åƒè·¯å¾„ï¼ˆå¦‚æœä¸º None åˆ™ä¸ä¿å­˜ï¼‰
            score_threshold: åˆ†æ•°é˜ˆå€¼
        
        Returns:
            ç»˜åˆ¶åçš„å›¾åƒ
        """
        # æ¨ç†
        results = self.infer_single(image_path, score_threshold)
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        draw = ImageDraw.Draw(image)
        
        # å°è¯•åŠ è½½å­—ä½“
        try:
            font = ImageFont.truetype("arial.ttf", 20)
        except:
            font = ImageFont.load_default()
        
        # ç»˜åˆ¶æ£€æµ‹æ¡†
        for det in results['detections']:
            x1, y1, x2, y2 = det['bbox']
            label = det['category_name']
            confidence = det['confidence']
            color = self.COLORS.get(label, 'yellow')
            
            # ç»˜åˆ¶çŸ©å½¢
            draw.rectangle([x1, y1, x2, y2], outline=color, width=3)
            
            # ç»˜åˆ¶æ ‡ç­¾
            text = f"{label} {confidence:.2f}"
            text_bbox = draw.textbbox((x1, y1), text, font=font)
            text_width = text_bbox[2] - text_bbox[0]
            text_height = text_bbox[3] - text_bbox[1]
            
            # æ ‡ç­¾èƒŒæ™¯
            draw.rectangle([x1, y1 - text_height - 5, x1 + text_width + 5, y1], 
                          fill=color)
            # æ ‡ç­¾æ–‡æœ¬
            draw.text((x1 + 2, y1 - text_height - 3), text, fill='white', font=font)
        
        # ä¿å­˜ç»“æœ
        if output_path:
            image.save(output_path)
            print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {output_path}")
        
        return image
    
    def evaluate_on_coco(self, coco_json_path: str, image_dir: str) -> Dict:
        """
        åœ¨ COCO æ•°æ®é›†ä¸Šè¯„ä¼°
        
        Args:
            coco_json_path: COCO æ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶
            image_dir: å›¾åƒç›®å½•
        
        Returns:
            è¯„ä¼°æŒ‡æ ‡
        """
        print("\nğŸ“Š åœ¨ COCO æ•°æ®é›†ä¸Šè¯„ä¼°...")
        
        # åŠ è½½ COCO æ ‡æ³¨
        coco_gt = coco.COCO(coco_json_path)
        
        # æ¨ç†ç”Ÿæˆé¢„æµ‹ç»“æœ
        results = []
        image_ids = coco_gt.getImgIds()
        
        for img_id in tqdm(image_ids, desc="æ¨ç†ä¸­"):
            img_info = coco_gt.loadImgs(img_id)[0]
            img_path = os.path.join(image_dir, img_info['file_name'])
            
            try:
                inference_result = self.infer_single(img_path)
                
                for det in inference_result['detections']:
                    x1, y1, x2, y2 = det['bbox']
                    w = x2 - x1
                    h = y2 - y1
                    
                    results.append({
                        'image_id': img_id,
                        'category_id': det['category'],
                        'bbox': [x1, y1, w, h],
                        'score': det['confidence']
                    })
            except Exception as e:
                print(f"âš ï¸  å¤„ç†å¤±è´¥: {img_path} - {e}")
        
        # COCO è¯„ä¼°
        coco_dt = coco_gt.loadRes(results)
        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')
        coco_eval.evaluate()
        coco_eval.accumulate()
        
        print("\n" + "=" * 70)
        coco_eval.summarize()
        print("=" * 70)
        
        return {
            'mAP': coco_eval.stats[0],
            'mAP_50': coco_eval.stats[1],
            'mAP_75': coco_eval.stats[2],
            'mAP_small': coco_eval.stats[3],
            'mAP_medium': coco_eval.stats[4],
            'mAP_large': coco_eval.stats[5],
        }


def main():
    parser = argparse.ArgumentParser(description='ç›®æ ‡æ£€æµ‹æ¨ç†è„šæœ¬')
    
    parser.add_argument(
        '--checkpoint',
        type=str,
        required=True,
        help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„'
    )
    parser.add_argument(
        '--image',
        type=str,
        default=None,
        help='å•å¼ å›¾åƒæ¨ç†è·¯å¾„'
    )
    parser.add_argument(
        '--image-dir',
        type=str,
        default=None,
        help='æ‰¹é‡æ¨ç†çš„å›¾åƒç›®å½•'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='./inference_results',
        help='è¾“å‡ºç›®å½•'
    )
    parser.add_argument(
        '--conf-threshold',
        type=float,
        default=0.5,
        help='ç½®ä¿¡åº¦é˜ˆå€¼'
    )
    parser.add_argument(
        '--gpu',
        type=int,
        default=0,
        help='GPU ID'
    )
    parser.add_argument(
        '--visualize',
        action='store_true',
        help='ä¿å­˜å¯è§†åŒ–ç»“æœ'
    )
    parser.add_argument(
        '--evaluate',
        type=str,
        default=None,
        help='COCO æ ‡æ³¨æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºè¯„ä¼°ï¼‰'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–æ¨ç†å¼•æ“
    device = f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu'
    inferencer = DetectionInference(args.checkpoint, device, args.conf_threshold)
    
    # å•å¼ å›¾åƒæ¨ç†
    if args.image:
        print(f"\nğŸ–¼ï¸  æ¨ç†å•å¼ å›¾åƒ: {args.image}")
        results = inferencer.infer_single(args.image)
        
        print(f"\nğŸ“Š æ£€æµ‹åˆ° {len(results['detections'])} ä¸ªç›®æ ‡:")
        for det in results['detections']:
            print(f"  - {det['category_name']}: {det['confidence']:.2%}")
        
        # ä¿å­˜ç»“æœ
        output_json = os.path.join(args.output_dir, 'single_result.json')
        with open(output_json, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"âœ… ç»“æœå·²ä¿å­˜: {output_json}")
        
        # å¯è§†åŒ–
        if args.visualize:
            output_image = os.path.join(args.output_dir, 'single_result_viz.png')
            inferencer.draw_predictions(args.image, output_image)
    
    # æ‰¹é‡æ¨ç†
    if args.image_dir:
        print(f"\nğŸ“ æ‰¹é‡æ¨ç†ç›®å½•: {args.image_dir}")
        output_json = os.path.join(args.output_dir, 'batch_results.json')
        all_results = inferencer.infer_batch(args.image_dir, output_json)
        
        total_detections = sum(len(r['detections']) for r in all_results)
        print(f"\nâœ… æ€»å…±æ£€æµ‹åˆ° {total_detections} ä¸ªç›®æ ‡")
    
    # åœ¨ COCO æ•°æ®é›†ä¸Šè¯„ä¼°
    if args.evaluate:
        image_dir = args.image_dir or args.image or args.output_dir
        inferencer.evaluate_on_coco(args.evaluate, image_dir)
    
    print(f"\nâœ… æ¨ç†å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {args.output_dir}")


if __name__ == '__main__':
    main()
